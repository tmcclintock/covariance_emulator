{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emulating $\\xi_+$-$\\xi_-$-GGL covariances\n",
    "\n",
    "The \"key project\" in DES is the combined probes analysis. For DES Y1, this was the 3x2pt analysis, which consisted of three 2-point functions (hence the name). There was used a corresponding covariance matrix between these probes. In this notebook, we will build an emulator for just the $\\xi_+$-$\\xi_-$-GGL ($\\gamma$) covariance from a set of 25 covariances computed by Tim Eifler in a 10 dimensional parameter space (cosmology + 5 tomographic biases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import covariance_emulator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc(\"font\", size=14, family=\"serif\")\n",
    "#plt.rc(\"text\", usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the domain locations, or locations in parameter space\n",
    "parameters = np.loadtxt(\"cosmo_parameters.txt\")\n",
    "print(parameters.shape)\n",
    "#Load in the covariances\n",
    "covs = np.load(\"gaussian_xipximgamma_sub_INVERSEcovs_withcut.npy\")\n",
    "print(covs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the correlation matrix of the first\n",
    "def corr_from_cov(cov):\n",
    "    D = np.diag(np.sqrt(cov.diagonal()))\n",
    "    Di = np.linalg.inv(D)\n",
    "    return np.dot(Di, np.dot(cov, Di))\n",
    "\n",
    "def view_corr(cov, lncov=False):\n",
    "    R = corr_from_cov(cov)\n",
    "    fig, ax = plt.subplots()\n",
    "    if lncov:\n",
    "        R = np.log(np.fabs(cov))\n",
    "    im = ax.imshow(R, interpolation=\"nearest\", origin=\"lower\")\n",
    "    plt.colorbar(im)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split off the last covariance matrix\n",
    "test_cov = covs[-1]\n",
    "test_parameters = parameters[-1]\n",
    "covs = covs[:-1]\n",
    "parameters = parameters[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an emulator\n",
    "NPC_D = 10\n",
    "NPC_L = 10\n",
    "#Emu = covariance_emulator.CovEmu(parameters, covs, NPC_D=NPC_D, NPC_L=NPC_L)\n",
    "#Cpredicted = Emu.predict(test_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding an optimal emulator\n",
    "\n",
    "The covariance emulator built above was done with the default configuration with a few principle components, but it actually has a few knobs to turn. We can control not only the number of principle components for D and L (`NPC_D, NPC_L`), but we can also create and pass in `george` kernels for both `D` and `L`. In the next cell, we will look over all reasonable options, and figure out which emulator setup is the best (but we keep the number of principle components fixed for now).\n",
    "\n",
    "Our method is the following:\n",
    "1. Take the test covariance matrix $C_{\\rm true}$ and draw from a multivariate normal in order to obtain a realization of the noise $d$.\n",
    "2. Compute $\\chi^2 = d^TC_{\\rm emu}^{-1}d$ using the inverse of the emulated covariance matrix.\n",
    "3. Repeat steps 1-2 thousands of times, recording all $\\chi^2$s.\n",
    "4. Histogram the $\\chi^2$ values and plot them against the expected distribution given the number of degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a covariance matrix, make realizations of the noise, and then find the optimal kernel set up\n",
    "def best_kernel_for_C(C, N_samples=1000):\n",
    "    dof = len(C)\n",
    "    means = np.zeros(dof)\n",
    "    chi2s = np.zeros(N_samples)\n",
    "    noise_realizations = np.array([np.random.multivariate_normal(means, C) for i in range(N_samples)])\n",
    "    import george.kernels as kernels\n",
    "    kerns = [kernels.ExpSquaredKernel]#, kernels.Matern52Kernel, kernels.Matern32Kernel]\n",
    "    names = [\"Exp2\"]#, \"Mat52\", \"Mat32\"]\n",
    "    Npars = len(parameters[0])\n",
    "    metric_guess = np.std(parameters, 0)\n",
    "    #Loop over kernel combinations and compute the chi2 shift\n",
    "    best_shift = 1e99\n",
    "    best_kernels = None\n",
    "    for nameD, kd in zip(names, kerns):\n",
    "        kernel_D = 1.*kd(metric=metric_guess, ndim=Npars)\n",
    "        for nameL, kl in zip(names, kerns):\n",
    "            kernel_L = 1.*kd(metric=metric_guess, ndim=Npars)\n",
    "            Emu = covariance_emulator.CovEmu(parameters, covs, NPC_D=NPC_D, NPC_L=NPC_L, \n",
    "                                             kernel_D = kernel_D, kernel_lp = kernel_L)\n",
    "            shift = 0\n",
    "            try:\n",
    "                Cpredicted = Emu.predict(test_parameters)\n",
    "                iCpredicted = np.linalg.inv(Cpredicted)\n",
    "            except np.linalg.LinAlgError:\n",
    "                shift = 1e99\n",
    "            else:\n",
    "                for i in range(N_samples):\n",
    "                    chi2s[i] = np.dot(noise_realizations[i], np.dot(iCpredicted, noise_realizations[i]))\n",
    "                    shift = np.mean(chi2s) - dof\n",
    "            if shift < best_shift and shift > 0:\n",
    "                best_shift = shift\n",
    "                best_name = \"%s %s\"%(nameD, nameL)\n",
    "                best_kernels = [kernel_D, kernel_L]\n",
    "            print(\"%s %s: %e / %d\"%(nameD, nameL, shift, dof))\n",
    "    print(\"Best combination: %s\"%best_name)\n",
    "    print(\"\\tshift/dof = %e / %d\"%(best_shift, dof))\n",
    "    return best_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_kernels = best_kernel_for_C(test_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's visualize\n",
    "kernel_D, kernel_L = best_kernels\n",
    "#kernel_L = 1.*kernels.Matern32Kernel(metric=metric_guess, ndim=Npars)\n",
    "Emu = covariance_emulator.CovEmu(parameters, covs, NPC_D=NPC_D, NPC_L=NPC_L, \n",
    "                                 kernel_D = kernel_D, kernel_lp = kernel_L)\n",
    "Cpredicted = Emu.predict(test_parameters)\n",
    "view_corr(Cpredicted)\n",
    "plt.title(r\"$\\xi_+\\xi_-\\gamma$ cut\")\n",
    "#plt.savefig(\"predicted_cov.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_corr(test_cov)\n",
    "plt.title(r\"$\\xi_+\\xi_-\\gamma$ cut\")\n",
    "#plt.savefig(\"true_cov.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_var = test_cov.diagonal()\n",
    "emu_var = Cpredicted.diagonal()\n",
    "frac_diff = (true_var - emu_var) / true_var\n",
    "\n",
    "fig, ax = plt.subplots(ncols=1, nrows=2, sharex=True)\n",
    "ax[0].plot(true_var, c='k', label='True variance')\n",
    "ax[0].plot(emu_var, c='r', label='Emulated variance')\n",
    "ax[1].plot(frac_diff, c='k')\n",
    "ax[0].set_yscale('log')\n",
    "ax[1].set_ylabel(r\"Fractional difference\")\n",
    "ax[1].set_xlabel(r\"Bin number\")\n",
    "#fig.savefig(\"scale_issue.png\", dpi=300, bbox_inches=\"tight\")\n",
    "#ax[1].set_ylim(-2.5, 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the emulator performance\n",
    "\n",
    "One of the best ways to assess the performance of the emulator is to directly compare the true covariance to the emulated covariance. In the next cell, I will draw realizations of the noise from the true covariance, and compute $\\chi^2$ values of these noises compared agains the emulated covariance. Then, by checking this against the expected distribution, we can see the performance of the emulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function where we input two covariances, and get back out a list of chi2s\n",
    "def get_chi2s_between_Cs(C1, C2, N_samples=1000):\n",
    "    means = np.zeros(len(C1))\n",
    "    chi2s = np.zeros(N_samples)\n",
    "    iC2 = np.linalg.inv(C2)\n",
    "    for i in range(N_samples):\n",
    "        x = np.random.multivariate_normal(means, C1)\n",
    "        chi2s[i] = np.dot(x, np.dot(iC2, x))\n",
    "    return chi2s\n",
    "\n",
    "dof = len(test_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2s = get_chi2s_between_Cs(test_cov, test_cov)\n",
    "plt.hist(chi2s, density=True, bins=100)\n",
    "x = np.linspace(min(chi2s)*0.97, 1.03*max(chi2s), 1000)\n",
    "plt.plot(x, stats.chi2.pdf(x, dof))\n",
    "plt.title(r\"$C_{\\rm true}$ vs $C_{\\rm true}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2s = get_chi2s_between_Cs(test_cov, Cpredicted, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(chi2s, density=True, bins=100)\n",
    "x = np.linspace(min(chi2s)*0.97, 1.03*max(chi2s), 1000)\n",
    "#x = np.linspace(300, 800, 1000)\n",
    "plt.plot(x, stats.chi2.pdf(x, dof))\n",
    "plt.title(r\"$C_{\\rm true}$ vs $C_{\\rm emu}$\")\n",
    "plt.xlabel(r\"$\\chi^2$\")\n",
    "plt.axvline(dof, color=\"k\", ls=\"--\")\n",
    "ax = plt.gca()\n",
    "#ax.text(0.7, 0.5, r\"$\\chi2=d^TC^{-1}d$\", transform=ax.transAxes)\n",
    "print(\"Chi2/dof shift = %.2f / %d\"%(np.mean(chi2s) - dof, dof))\n",
    "plt.savefig(\"chi2_realizations.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emulated covariance vs. any random covariance\n",
    "\n",
    "In fiducial analyses, and as has been suggested in the literature, we should be \"fine\" with neglecting parameter dependence in the covariance matrix. We can test this easily, by doing the chi2 distribution comparison between the test covariance matrix and the covariances we have on hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2s = get_chi2s_between_Cs(test_cov, covs[0])\n",
    "plt.hist(chi2s, density=True, bins=100)\n",
    "x = np.linspace(90, 300, 1000)\n",
    "plt.plot(x, stats.chi2.pdf(x, dof))\n",
    "plt.title(r\"$C_{\\rm true}$ vs $C_{\\rm 0}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try looping over a few and comparing\n",
    "x = np.linspace(90, 300, 1000)\n",
    "plt.plot(x, stats.chi2.pdf(x, dof))\n",
    "\n",
    "for i in [0, 10, 20]:\n",
    "    chi2s = get_chi2s_between_Cs(test_cov, covs[0], 1000)\n",
    "    plt.hist(chi2s, density=True, bins=100, alpha=0.3, label=r\"$C_{%d}$\"%i)\n",
    "    print(\"Chi2/dof shift = %.2f / %d\"%(np.mean(chi2s) - dof, dof))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for 200 degrees of freedom, using any old covariance matrix can shift $chi^2$ by about 28/200, while just using the emulator is essentially perfect. Thus, it is a clear improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
